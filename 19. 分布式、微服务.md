## 分布式、微服务知识点

> 参考《高可用可伸缩微服务架构》

###  领域驱动有了解吗？什么是领域驱动模型？

### JWT有了解吗，什么是JWT

### 说说如何设计一个良好的 API

### 说说 CAP 定理、BASE 理论

CAP: 

 C(一致性  Consistency)，所有节点上的数据时刻保持一致

 A(可用性 Avaliability)，每个请求都能够收到一个响应，无论响应成功或者失败

P(分区容错 Partition-tolerance)，系统出现脑裂以后，可能导致某些server 与集群中的机器失去联系

BASE： 

XA事务虽然可以保证数据库在分布式系统下的ACID特性，但会带来性能方面的影响

eBay提出了BASE理论

Basically available：数据库采用分片模式，把100w的用户数据分布在5个实例上，如果破坏了其中一个实例，仍然可以保证80%的用户可用

Soft-state：在基于client-server模式的系统中，server端是否有状态，决定了系统是否具备良好的水平扩展、负载均衡、故障恢复等特性。Server端承诺会维护client端状态数据，这个状态仅维持一小段时间，这段时间以后，server端会丢弃这个状态，恢复正常状态

Eventually consistent：数据最终一致性



### 微服务与 SOA 的区别

SOA即面向服务架构，关注点是服务，现有的分布式服务化技术有Dubbo等

1. 微服务是一种经过改良架构设计的SOA解决方案，是面向服务的交互方案
2. 微服务更趋向于以自治的方式产生价值
3. 微服务与敏捷开发的思想高度结合在一起，服务的定义更加清晰，同时减少了企业ESB开发的复杂性
4. 微服务是SOA思想的一种提炼
5. SOA是重ESB,微服务是轻网关

### 如何拆分服务、水平分割、垂直分割

### 如何应对微服务的链式调用异常

### 如何快速追踪与定位问题

### 如何保证微服务的安全、认证



### 如何保证接口的幂等性

1. 乐观锁，使用版本号

2. 唯一索引，可以把作为唯一索引的键单独称为一个表，作为去重表

3. 分布式锁，使用setnx，如果setnx返回0就代表重复请求，同时在业务逻辑处理完成后，删除缓存

    

### 分布式服务接口请求的顺序性如何保证？

首先，一般来说，个人建议是，你们从业务逻辑上设计的这个系统最好是不需要这种顺序性的保证，因为一旦引入顺序性保障，比如使用**分布式锁**，会**导致系统复杂度上升**，而且会带来**效率低下**，热点数据压力过大等问题。

下面我给个我们用过的方案吧，简单来说，首先你得用 dubbo 的一致性 hash 负载均衡策略，将比如某一个订单 id 对应的请求都给分发到某个机器上去，接着就是在那个机器上，因为可能还是多线程并发执行的，你可能得立即将某个订单 id 对应的请求扔一个**内存队列**里去，强制排队，这样来确保他们的顺序性。

[<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gaf4mkyhxmj30jp0lkwem.jpg" alt="distributed-system-request-sequence" style="zoom:50%;" />

但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成**热点**怎么办？解决这些问题又要开启后续一连串的复杂技术方案......曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？

最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是其它什么，避免这种问题的产生。



### 说说分布式一致性的实现方案

1. 两阶段提交

    分准备阶段、提交阶段，由事务管理协调器发起

    准备阶段：事务管理器向参与者发起指令，参与者评估自己的状态，如果参与者评估指令可以完成，则会写redo或者undo日志，然后锁定资源，执行操作，但并不提交。如果其中一个参与者返回准备准备失败，则协调者向参与者发起中止指令，参与者取消已经变更的事务，执行undo日志，释放锁定的资源

    提交阶段：如果每个参与者明确返回准备成功，也就是预留资源和执行操作成功，则协调者向参与者发起提交指令，参与者提交资源变更的事务，释放锁定的资源；

    缺点：

    阻塞，对于任何一次指令都必须收到明确的响应，才会继续下一步，否则处于阻塞状态，占用的资源被一直锁定，不会释放

    单点故障，协调者宕机，参与者没有协调者指挥，会一直阻塞，需要自己实现协调者选举

    脑裂，协调者发送指令，有的参与者可能会没有接收到指令，导致多个参与者事务状态不一致

2. 三阶段提交

    是二阶段提交的改进版本，通过超时机制解决了阻塞问题

    询问阶段：协调者询问参与者是否可以完成指令，协调者回复是或者不是，不做真正的操作，如果超时会导致中止

    准备阶段、提交阶段：与二阶段相同

3. TCC补偿事务 

    将一个任务拆分成Try、Confirm、Cancle三个步骤，主业务先发起请求执行Try，如果没有问题，则提交任务到TCC事务管理器，由事务管理器执行Confirm，如果出现问题，再执行逆操作Cancel

    优点：解决了阻塞问题，通过TCC自动化Cancel降低了不一致的情况

    缺点：实现还是臃肿，在极端情况下会出现不一致和脑裂问题

4. RocketMQ事务

    系统A发送一个事务消息到MQ，MQ会反馈消息接收成功，系统A会收到消息接收成功回调，此时系统A执行本地事务，执行成功后向MQ发送确认消息，否则向MQ取消消息，MQ收到确认消息后，消费系统B就能够接收到该事务消息，执行操作

### 说说达到最终一致性的方案

1. 查询模式，通过查询了解调用服务的最终处理情况，决定下一步做什么
2. 补偿模式，有了查询模式，就可以知道服务所在状态，通过补偿模式修正操作
3. 定时校对模式

### 缓存、数据库一致性方案

1. 更新操作，先删除缓存，再修改数据库，如果数据库失败，数据库中的还是旧数据，缓存是空的，在查询的时候，发现缓存是空的，就会从数据库取数据，然后更新到缓存中

2. 方案一在高并发下，先删除了缓存，但数据库还没修改成功，此时读请求过来，发现没有缓存，就会去查询数据库，放入缓存，随后更新数据库操作完成了，此时缓存中的数据是旧数据，与数据库中的数据不一致。

    方案如下：

    在请求的时候，发现没有缓存，就发一个更新操作，到JVM队列，主线程循环判断缓存是否更新，并设置超时时间，如果超时，就直接取数据库数据，返回旧数据。JVM队列中，如果之前已经有更新操作，自动丢弃后面的更新操作，防止频繁更新。

    如果实例服务是分布式部署，需要将同一请求路由到同一个实例，可以通过某个请求参数的hash路由，也可以通过Nginx的hash路由功能

### 如何把系统不停机迁移到分库分表的

#### 1、面试题

现在有一个未分库分表的系统，未来要分库分表，如何设计才可以让系统从未分库分表动态切换到分库分表上？

#### 2、面试官心里分析

你看看，你现在已经明白为啥要分库分表了，你也知道常用的分库分表中间件了，你也设计好你们如何分库分表的方案了（水平拆分、垂直拆分、分表），那问题来了，你接下来该怎么把你那个单库单表的系统给迁移到分库分表上去？

所以这都是一环扣一环的，就是看你有没有全流程经历过这个过程。

#### 友情提示：

假设，你现有有一个单库单表的系统，在线上在跑，假设单表有600万数据

3个库，每个库里分了4个表，每个表要放50万的数据量

假设你已经选择了一个分库分表的数据库中间件，sharding-jdbc，mycat，都可以

你怎么把线上系统平滑地迁移到分库分表上面去

sharding-jdbc：自己上官网，找一个官网最基本的例子，自己写一下，试一下，跑跑看，是非常简单的

mycat：自己上官网，找一个官网最基本的例子，自己写一下，试一下看看

1个小时以内就可以搞定了

#### 3、面试题剖析

这个其实从low到高大上有好几种方案，我们都玩儿过，我都给你说一下

##### （1）停机迁移方案

我先给你说一个最low的方案，就是很简单，大家伙儿凌晨12点开始运维，网站或者app挂个公告，说0点到早上6点进行运维，无法访问。。。。。。

接着到0点，停机，系统挺掉，没有流量写入了，此时老的单库单表数据库静止了。然后你之前得写好一个导数的一次性工具，此时直接跑起来，然后将单库单表的数据哗哗哗读出来，写到分库分表里面去。

导数完了之后，就ok了，修改系统的数据库连接配置啥的，包括可能代码和SQL也许有修改，那你就用最新的代码，然后直接启动连到新的分库分表上去。

验证一下，ok了，完美，大家伸个懒腰，看看看凌晨4点钟的北京夜景，打个滴滴回家吧！

但是这个方案比较low，谁都能干，我们来看看高大上一点的方案。

##### （2）双写迁移方案

这个是我们常用的一种迁移方案，比较靠谱一些，不用停机，不用看北京凌晨4点的风景

简单来说，就是在线上系统里面，之前所有写库的地方，增删改操作，都除了对老库增删改，都加上对新库的增删改，这就是所谓双写，同时写俩库，老库和新库。

然后系统部署之后，新库数据差太远，用之前说的导数工具，跑起来读老库数据写新库，写的时候要根据gmt_modified这类字段判断这条数据最后修改的时间，除非是读出来的数据在新库里没有，或者是比新库的数据新才会写。

接着导万一轮之后，有可能数据还是存在不一致，那么就程序自动做一轮校验，比对新老库每个表的每条数据，接着如果有不一样的，就针对那些不一样的，从老库读数据再次写。反复循环，直到两个库每个表的数据都完全一致为止。

接着当数据完全一致了，就ok了，基于仅仅使用分库分表的最新代码，重新部署一次，不就仅仅基于分库分表在操作了么，还没有几个小时的停机时间，很稳。所以现在基本玩儿数据迁移之类的，都是这么干了。



### 如何设计可以动态扩容缩容的分库分表方案？

#### 1、面试题

如何设计可以动态扩容缩容的分库分表方案？

#### 2、面试官心里分析

（1）选择一个数据库中间件，调研、学习、测试；
（2）设计你的分库分表的一个方案，你要分成多少个库，每个库分成多少个表，3个库每个库4个表；
（3）基于选择好的数据库中间件，以及在测试环境建立好的分库分表的环境，然后测试一下能否正常进行分库分表的读写；
（4）完成单库单表到分库分表的迁移，双写方案；
（5）线上系统开始基于分库分表对外提供服务；
（6）扩容了，扩容成6个库，每个库需要12个表，你怎么来增加更多库和表呢？

这个是你必须面对的一个事儿，就是你已经弄好分库分表方案了，然后一堆库和表都建好了，基于分库分表中间件的代码开发啥的都好了，测试都ok了，数据能均匀分布到各个库和各个表里去，而且接着你还通过双写的方案咔嚓一下上了系统，已经直接基于分库分表方案在搞了。

那么现在问题来了，你现在这些库和表又支撑不住了，要继续扩容咋办？这个可能就是说你的每个库的容量又快满了，或者是你的表数据量又太大了，也可能是你每个库的写并发太高了，你得继续扩容。

这都是玩儿分库分表线上必须经历的事儿

#### 3、面试题剖析

##### （1）停机扩容

这个方案就跟停机迁移一样，步骤几乎一致，唯一的一点就是那个导数的工具，是把现有库表的数据抽出来慢慢倒入到新的库和表里去。但是最好别这么玩儿，有点不太靠谱，因为既然分库分表就说明数据量实在是太大了，可能多达几亿条，甚至几十亿，你这么玩儿，可能会出问题。

从单库单表迁移到分库分表的时候，数据量并不是很大，单表最大也就两三千万。

写个工具，多弄几台机器并行跑，1小时数据就导完了。

3个库+12个表，跑了一段时间了，数据量都1亿~2亿了。光是导2亿数据，都要导个几个小时，6点，刚刚导完数据，还要搞后续的修改配置，重启系统，测试验证，10点才可以搞完。

##### （2）优化后的方案

一开始上来就是32个库，每个库32个表，1024张表

我可以告诉各位同学说，这个分法，第一，基本上国内的互联网肯定都是够用了，第二，无论是并发支撑还是数据量支撑都没问题。

每个库正常承载的写入并发量是1000，那么32个库就可以承载32 * 1000 = 32000的写并发，如果每个库承载1500的写并发，32 * 1500 = 48000的写并发，接近5万/s的写入并发，前面再加一个MQ，削峰，每秒写入MQ 8万条数据，每秒消费5万条数据。

有些除非是国内排名非常靠前的这些公司，他们的最核心的系统的数据库，可能会出现几百台数据库的这么一个规模，128个库，256个库，512个库。

1024张表，假设每个表放500万数据，在MySQL里可以放50亿条数据。

每秒的5万写并发，总共50亿条数据，对于国内大部分的互联网公司来说，其实一般来说都够了。

谈分库分表的扩容，第一次分库分表，就一次性给他分个够，32个库，1024张表，可能对大部分的中小型互联网公司来说，已经可以支撑好几年了。

一个实践是利用32 * 32来分库分表，即分为32个库，每个库里一个表分为32张表。一共就是1024张表。根据某个id先根据32取模路由到库，再根据32取模路由到库里的表。

刚开始的时候，这个库可能就是逻辑库，建在一个数据库上的，就是一个mysql服务器可能建了n个库，比如16个库。后面如果要拆分，就是不断在库和mysql服务器之间做迁移就可以了。然后系统配合改一下配置即可。

比如说最多可以扩展到32个数据库服务器，每个数据库服务器是一个库。如果还是不够？最多可以扩展到1024个数据库服务器，每个数据库服务器上面一个库一个表。因为最多是1024个表么。

这么搞，是不用自己写代码做数据迁移的，都交给dba来搞好了，但是dba确实是需要做一些库表迁移的工作，但是总比你自己写代码，抽数据导数据来的效率高得多了。

哪怕是要减少库的数量，也很简单，其实说白了就是按倍数缩容就可以了，然后修改一下路由规则。

对2 ^ n取模

orderId 模 32 = 库
orderId / 32 模 32 = 表

259 3 8
1189 5 5
352 0 11
4593 17 15

1、设定好几台数据库服务器，每台服务器上几个库，每个库多少个表，推荐是32库 * 32表，对于大部分公司来说，可能几年都够了；

2、路由的规则，orderId 模 32 = 库，orderId / 32 模 32 = 表；

3、扩容的时候，申请增加更多的数据库服务器，装好mysql，倍数扩容，4台服务器，扩到8台服务器，16台服务器；

4、由dba负责将原先数据库服务器的库，迁移到新的数据库服务器上去，很多工具，库迁移，比较便捷；

5、我们这边就是修改一下配置，调整迁移的库所在数据库服务器的地址；

6、重新发布系统，上线，原先的路由规则变都不用变，直接可以基于2倍的数据库服务器的资源，继续进行线上系统的提供服务。

### 